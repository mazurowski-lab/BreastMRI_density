import argparse
import os
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"
import pandas as pd
import shutil
from tqdm import tqdm
import sys
import numpy as np
from dataset import BreastDataset
from utils.utils import DiceLoss, dice_coeff
from model import get_model

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchio as tio
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP

from monai.losses import DiceCELoss, GeneralizedDiceLoss
from monai.metrics import DiceMetric

from monai.networks.nets import SwinUNETR

from engine import eval_volume, eval_full_volume
import json
from preprocess import *
from monai.data import (
    ThreadDataLoader,
    CacheDataset,
    load_decathlon_datalist,
    decollate_batch,
    set_track_meta,
)

from torch_ema import ExponentialMovingAverage as EMA
import json

# os.environ["CUDA_VISIBLE_DEVICES"]="5"

if torch.cuda.is_available():
    print(f"CUDA available. Found {torch.cuda.device_count()} GPU(s)")
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")
else:
    print("CUDA not available. Running on CPU")

def main(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if args.vis:
        writer = SummaryWriter(os.path.join('runs', args.name))
    else:
        writer = None

    if not os.path.exists(args.preprocessed_dir):
        os.makedirs(args.preprocessed_dir)
    if not os.path.exists(args.save_dir):
        os.makedirs(args.save_dir)
    
    if args.json_generate_only_and_run == True:
        Process_df = pd.read_csv('buffer/Process_df.csv') # Process_df.csv is generated by data_preprocess
        generate_json(args.preprocessed_dir, Process_df)
        
    if args.preprocess == True:
        sequence_df_path = pd.read_csv(args.sequence_df_path)
        data_preprocess(sequence_df_path, args.preprocessed_dir, args.pre_image_generate,args.eval_task)
        Process_df = pd.read_csv('buffer/Process_df.csv') # Process_df.csv is generated by data_preprocess
        generate_json(args.preprocessed_dir, Process_df)

    if args.Process_df_Genrate_Only ==True:
        return
        
    if args.Item_Features_Genrate_Only ==True:
        Process_df = pd.read_csv('buffer/Process_df.csv') # Process_df.csv is generated by data_preprocess
        generate_json(args.preprocessed_dir, Process_df)
        return
        
    if args.patch_size > 0:
        resize_transform = None
    else:
        resize_transform = tio.Compose([tio.Resize(args.full_eval_size), ])

    unlabel_dataset = BreastDataset(image_dir=args.preprocessed_dir,
                                    mask_dir=None,
                                    pre_load=False,
                                    transforms=resize_transform,
                                    phase='generation')
    unlabel_loader = DataLoader(unlabel_dataset,
                                batch_size=args.batch_size,
                                shuffle=False,
                                num_workers=0,
                                drop_last=False)

    # Get model
    model = get_model(args).to(device)
    model = nn.DataParallel(model)
    model.load_state_dict(torch.load(args.load_path, map_location=device))
    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)

    with torch.no_grad():
        if args.patch_size > 0:
            eval_volume(model, unlabel_loader, args.patch_size, device, save_dir=args.save_dir)
        else:
            eval_full_volume(model, unlabel_loader, device, args.patch_size, args.save_dir)

    print('Finish generate mask!')


def get_args():
    parser = argparse.ArgumentParser(description='Foundation model on breast MRI')
    parser.add_argument('--lr', type=float, default=3e-4, help='learning rate')
    parser.add_argument('--image_dir', default=None)
    parser.add_argument('--preprocessed_dir',
                        default= 'None', help='unlabeled image dir, used for SSL')
    parser.add_argument('--save_dir', default=r'demo_result')
    parser.add_argument('--mask_dir', default=None)
    parser.add_argument('--phase', type=str, default='train')
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch_size', type=int, default=1)
    parser.add_argument('--name', type=str, default='3d_volume', help='name of the saved model')
    parser.add_argument('--vis', type=bool, default=False, help='track training loss')
    parser.add_argument('--load_path', type=str, default=None, help='path to load model if specific')

    parser.add_argument('--patch_size', type=int, default=96, help='size of input patch')
    parser.add_argument('--model', type=str, default='vnet', help='model to use')
    parser.add_argument('--in_channel', type=int, default=1, help='number of in-channel')
    parser.add_argument('--out_channel', type=int, default=3, help='number of out-channel')

    parser.add_argument('--amp', type=bool, default=True, help='whether use automatic mixed precision')
    parser.add_argument("--local_rank", type=int, default=0, help="local rank")

    parser.add_argument("--preprocess", type=bool, default=False, help="If preprocess data")
    parser.add_argument("--sequence_df_path", type=bool, default=False, help="2 columns dataframe: paths of raw dicom sequences, and study names")
    parser.add_argument("--pre_image_generate", type=bool, default=False, help="if generate preprocessed image. If false, only generate Process_df")
    parser.add_argument("--full_eval_size", type=tuple, default=(256, 256, 128), help="full_eval_size")
    parser.add_argument("--eval_task", type=str, default='full_breast', help="full_breast or vessel_tissue")
    parser.add_argument("--Process_df_Genrate_Only", type=bool, default=False)
    parser.add_argument("--json_generate_only_and_run", type=bool, default=False)
    parser.add_argument("--Item_Features_Genrate_Only", type=bool, default=False)


    if 'ipykernel' in sys.argv[0] or 'ipython' in sys.argv[0]:
        args = parser.parse_args(args=[])
    else:
        args = parser.parse_args()
    return args



def generate_json(input_data_path, Process_df):
    # List all files in the directory
    file_names = [f for f in os.listdir(input_data_path) if os.path.isfile(os.path.join(input_data_path, f))]
    file_names.sort()
    #file_names = list(Process_df.filenames)
    ISFlip = list(Process_df.ISFlip)
    XYFlip = list(Process_df.XYFlip)
    Padded = list(Process_df.Padded)
    xy_size = list(Process_df.xy_size)
    z_size = list(Process_df.z_size)
    # Create a list of dictionaries, each containing the 'Path' and 'split' for each file
    data = {
        'path': file_names,
        'split': ['val' for _ in file_names],  # This creates a list of 'val' strings, one for each path
        'ISFlip': ISFlip,
        'XYFlip': XYFlip,
        'Padded': Padded,
        'xy_size': xy_size,
        'z_size': z_size
    }

    # Convert the list of dictionaries to a JSON string
    json_data = json.dumps(data, indent=4)
    # Define the JSON file name
    json_file = 'buffer/Item_Features.json'
    # Write the JSON string to a file
    with open(json_file, 'w') as f:
        f.write(json_data)



if __name__ == '__main__':

    args = get_args()

    args.preprocessed_dir = r'demo_preprocessed'
    args.save_dir = r'demo_result/dv'
    args.load_path = r'weight/vnet_dv_baseline_159_aug_48.pth'
    args.sequence_df_path = 'demo.csv'
    args.patch_size = 96
    args.out_channel = 3
    args.full_eval_size = (256, 256, 128)
    args.eval_task = 'vessel_tissue'

    # args.preprocessed_dir = r'demo_preprocessed'
    # args.save_dir = r'demo_result/breast'
    # args.load_path = r'weight/vnet_baseline_breast_full_9511.pth'
    # args.sequence_df_path = 'demo.csv'
    # args.patch_size = 0
    # args.out_channel = 2
    # args.full_eval_size = (256, 256, 128)
    # args.eval_task = 'full_breast'
    
    args.preprocess = True
    args.pre_image_generate = True

    args.Process_df_Genrate_Only = False
    args.json_generate_only_and_run = False
    args.Item_Features_Genrate_Only = False
    main(args)
